---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.11.4
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
# %config InlineBackend.figure_format = 'svg'
```

forked from [ritvikmath-TimeSeries-Analysis](https://github.com/ritvikmath/Time-Series-Analysis/blob/master/SARIMA%20Model.ipynb)  
[video-walkthrough by ritvik](https://www.youtube.com/watch?v=Al8m6K_stfA)

```{python jupyter={'outputs_hidden': True}, tags=c()}
# ! pip install pandas
# ! pip install numpy
# ! pip install matplotlib
# ! pip install statsmodels
```

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from pandas.plotting import register_matplotlib_converters
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.tsa.statespace.sarimax import SARIMAX
register_matplotlib_converters()
from time import time
```

# Catfish Sales Data

```{python}
def parser(s):
    return datetime.strptime(s, '%Y-%m-%d')
```

```{python}
#read data
catfish_sales = pd.read_csv('data/catfish.csv', parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)
```

`squeeze` argument reduces the any nan, null, 0 columns and can even return a `series` if it can be represented as a single columns datastructure.

```{python}
catfish_sales.head()
```

```{python}
#infer the frequency of the data
catfish_sales = catfish_sales.asfreq(pd.infer_freq(catfish_sales.index))
```

[pd-docs-asfreq](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.asfreq.html)  
[pd-docs-infer_freq](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.infer_freq.html?highlight=infer_freq)

By reindexing with an inferred frequency of months, we can identify any missing values in the monthly data. I think it is not required in this particular dataset but is good practice.

```{python}
pd.infer_freq(catfish_sales.index)
```

MS referes to a frequency string [pd-docs-freq_str](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)

```{python}
catfish_sales.asfreq('D')
```

When using 'D' it organises the data for each day leaving any missing data as `NaN`

```{python}
start_date = datetime(1996,1,1)
end_date = datetime(2000,1,1)
lim_catfish_sales = catfish_sales[start_date:end_date]
```

We limit ourselves with a smaller dataset and don't see the rest so we can use this training dataset to predict the trend. We dont see the rest so we don't bias ourselves.

```{python}
fig, ax = plt.subplots(figsize=(10,4))
_ = ax.plot(lim_catfish_sales, color='black')
_ = ax.set(title='Catfish Sales in 1000s of Pounds', ylabel='Sales')
for year in range(start_date.year, end_date.year):
    _ = ax.axvline(pd.to_datetime(str(year)+'-01-01'), color='k', linestyle='--', alpha=0.2)
                   
# original
# plt.figure(figsize=(10,4))
# plt.plot(lim_catfish_sales)
# plt.title('Catfish Sales in 1000s of Pounds', fontsize=20)
# plt.ylabel('Sales', fontsize=16)
# for year in range(start_date.year,end_date.year):
#     plt.axvline(pd.to_datetime(str(year)+'-01-01'), color='k', linestyle='--', alpha=0.2)
```

There is a rising linear trend, with clear seasonal peaks and throughs between a 12 month time period.


## Remove the trend

```{python}
first_diff = lim_catfish_sales.diff(1)[1:]
```

```{python}
plt.figure(figsize=(10,4))
plt.plot(first_diff)
plt.title('Catfish Sales in 1000s of Pounds', fontsize=20)
plt.ylabel('Sales', fontsize=16)
for year in range(start_date.year,end_date.year):
    plt.axvline(pd.to_datetime(str(year)+'-01-01'), color='k', linestyle='--', alpha=0.2)
plt.axhline(0, color='k', linestyle='--', alpha=0.2)
```

# ACF


Auto-Correlation Function (ACF) performs pearson correlation across different lags of the same time-series. In the code below we auto-correlate the first 20 lags and plot as a bar chart. ACF gives us an overview of the direct and in-direct correlation.


`ax.set_axisbelow(True)` taken from [so-draw-grid-behind](https://stackoverflow.com/questions/1726391/matplotlib-draw-grid-lines-behind-other-graph-elements)

```{python}
acf_vals = acf(first_diff, fft=False)
num_lags = 20
# plt.bar(range(num_lags), acf_vals[:num_lags])

# my own
fig, ax = plt.subplots()
_ = ax.grid(color='lightgray')
_ = ax.set_axisbelow(True)
_ = ax.bar(range(num_lags), acf_vals[:num_lags], color='black', zorder=1)
_ = ax.set_xticks(range(num_lags+1))
_ = ax.set(title='ACF')
_ = ax.axhline(0.5, color='darkred', linestyle='-', alpha=0.5)
_ = ax.axhline(-0.5, color='darkred', linestyle='-', alpha=0.5)
```

## Based on ACF, we should start with a seasonal MA process

See the spike at Lag Order 12


# PACF


Partial Auto-Correlation Function is similar to the ACF but does not account for the in-direct influences of a value to another. I.e. Jan 2018's effect to Jan 2019 through Feb 2018 -...- Dec 2018, instead of the direct affect that comes from both being January (i.e 12 months apart to an anual period)).


`plt.rcParams['axes.axisbelow'] = True` taken from [so-draw-grid-behind](https://stackoverflow.com/questions/1726391/matplotlib-draw-grid-lines-behind-other-graph-elements)

```{python}
num_lags = 15
pacf_vals = pacf(first_diff, nlags=num_lags)
plt.bar(range(num_lags), pacf_vals[:num_lags], color='black')
plt.xticks(range(num_lags))
plt.grid(color='lightgray')
plt.axhline(1, color='darkred', alpha=0.5)
plt.axhline(-1, color='darkred', alpha=0.5)
plt.rcParams['axes.axisbelow'] = True
```

`[ ]` Make the graph above a little easier to read


## Based on PACF, we should start with a seasonal AR process


# Get training and testing sets

```{python}
train_end = datetime(1999,7,1)
test_end = datetime(2000,1,1)

train_data = lim_catfish_sales[:train_end]
test_data = lim_catfish_sales[train_end + timedelta(days=1):test_end]
```

# Fit the SARIMA Model

```{python}
my_order = (0,1,0)
my_seasonal_order = (1, 0, 1, 12)
# define model
model = SARIMAX(train_data, order=my_order, seasonal_order=my_seasonal_order)
```

```{python}
#fit the model
start = time()
model_fit = model.fit()
end = time()
print('Model Fitting Time:', end - start)
```

```{python}
#summary of the model
print(model_fit.summary())
```

```{python}
#get the predictions and residuals
predictions = model_fit.forecast(len(test_data))
predictions = pd.Series(predictions, index=test_data.index)
residuals = test_data - predictions
```

Residuals is the absolute error difference between the predicted and actual value.

```{python}
plt.figure(figsize=(10,4))
plt.plot(residuals, color='k')
plt.axhline(0, linestyle='--', color='k')
plt.title('Residuals from SARIMA Model', fontsize=20)
plt.ylabel('Error', fontsize=16)
plt.grid(True, color='lightgrey')
plt.rcParams['axes.axisbelow']=True
```

`[?]` Why might the residuals graph be shaped as it is?

```{python}
plt.figure(figsize=(10,4))

plt.plot(lim_catfish_sales)
plt.plot(predictions)

plt.legend(('Data', 'Predictions'), fontsize=16)

plt.title('Catfish Sales in 1000s of Pounds', fontsize=20)
plt.ylabel('Production', fontsize=16)
for year in range(start_date.year,end_date.year):
    plt.axvline(pd.to_datetime(str(year)+'-01-01'), color='k', linestyle='--', alpha=0.2)
```

```{python}
print('Mean Absolute Percent Error:', round(np.mean(abs(residuals/test_data)),4))
```

```{python}
print('Root Mean Squared Error:', np.sqrt(np.mean(residuals**2)))
```

# Using the Rolling Forecast Origin


We are predicting the next month with the all the knowlegde of the previous data so we only forcast a month ahead and repeat with knowledge of the new month to predict n+1 month.

We use $0$ to $n-1$ to predict month $n$ and then use the real value of $n$ to predict month $n+1$, we repeat that across all the way. 

```{python tags=c()}
rolling_predictions = test_data.copy()
for train_end in test_data.index:
    train_data = lim_catfish_sales[:train_end-timedelta(days=1)]
    model = SARIMAX(train_data, order=my_order, seasonal_order=my_seasonal_order)
    model_fit = model.fit()

    pred = model_fit.forecast()
    rolling_predictions[train_end] = pred
```

```{python}
rolling_residuals = test_data - rolling_predictions
```

```{python}
plt.figure(figsize=(10,4))
plt.plot(rolling_residuals, color='k')
plt.axhline(0, linestyle='--', color='k')
plt.title('Rolling Forecast Residuals from SARIMA Model', fontsize=20)
plt.ylabel('Error', fontsize=16)
plt.grid(color='lightgrey')
plt.rcParams['axes.axisbelow']=True
```

```{python}
plt.figure(figsize=(10,4))

plt.plot(lim_catfish_sales)
plt.plot(rolling_predictions)

plt.legend(('Data', 'Predictions'), fontsize=16)

plt.title('Catfish Sales in 1000s of Pounds', fontsize=20)
plt.ylabel('Production', fontsize=16)
for year in range(start_date.year,end_date.year):
    plt.axvline(pd.to_datetime(str(year)+'-01-01'), color='k', linestyle='--', alpha=0.2)
```

`[?]` Why do you think it underpredicted the sales for Dec 1999?  
`[>]` There is either missing data or the underestimation is being infuenced by the slump in sales around Dec 1996-1998

```{python}
print('Mean Absolute Percent Error:', round(np.mean(abs(rolling_residuals/test_data)),4))
```

```{python}
print('Root Mean Squared Error:', np.sqrt(np.mean(rolling_residuals**2)))
```
